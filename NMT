class NMTModel(tf.keras.Model):
    """
    NMTModel Class
    """
    def __init__(self, args, name='nmt_model'):
        """
        생성자
        :param args: Args 객체
        :param name: layer name
        """
        super().__init__(name=name)

        self.embedding = tf.keras.layers.Embedding(args.n_vocab, args.d_model, mask_zero=True)
        self.enc_lstm = tf.keras.layers.LSTM(units=args.d_model, return_state=True)
        self.dec_lstm = tf.keras.layers.LSTM(units=args.d_model, return_sequences=True)
        self.linear = tf.keras.layers.Dense(args.n_vocab)

    def call(self, inputs, training=False):
        """
        layer 실행
        :param inputs: tokens
        :return y_pred: tokens 예측 결과
        """
        enc_tokens, dec_tokens = inputs

        enc_hidden = self.embedding(enc_tokens)  # (bs, n_seq, d_model)
        enc_mask = self.embedding.compute_mask(enc_tokens)
        enc_hidden, enc_h_state, enc_c_state = self.enc_lstm(enc_hidden, mask=enc_mask)

        dec_hidden = self.embedding(dec_tokens)  # (bs, n_seq, d_model)
        dec_mask = self.embedding.compute_mask(dec_tokens)
        dec_hidden = self.dec_lstm(dec_hidden, initial_state=[enc_h_state, enc_c_state], mask=dec_mask)

        logits = self.linear(dec_hidden)
        return logits



def make_data(df, vocab, n_enc_seq, n_dec_seq):
    """
    chat 학습 데이터 생성
    :param df: data frame
    :param df: vocab
    :param n_enc_seq: number of encoder sequence
    :param n_dec_seq: number of decoder sequence
    :return enc_tokens: encoder token data
    :return dec_tokens: decoder token data
    :return dec_labels: decoder label data
    """
    n_enc_max = n_enc_seq
    n_dec_max = n_dec_seq - 1  # [BOS] or [EOS]
    # inputa & labels
    enc_tokens = []
    dec_tokens = []
    dec_labels = []
    # 데이터 생성
    for i, row in tqdm(df.iterrows(), total=len(df)):
        Q = row['Q']
        A = row['A']
        # tokenize
        tokens_q = vocab.encode_as_ids(Q)
        tokens_q = tokens_q[:n_enc_max]
        tokens_a = vocab.encode_as_ids(A)
        tokens_a = tokens_a[:n_dec_max]
        # enc_token
        enc_token = tokens_q
        enc_token += [0] * (n_enc_seq - len(enc_token))
        enc_tokens.append(enc_token)
        # dec_token
        dec_token = [vocab.bos_id()] + tokens_a
        dec_token += [0] * (n_dec_seq - len(dec_token))
        dec_tokens.append(dec_token)
        # dec_label
        dec_label = tokens_a + [vocab.eos_id()]
        dec_label += [0] * (n_dec_seq - len(dec_label))
        dec_labels.append(dec_label)

    # to numpy array
    enc_tokens = np.array(enc_tokens)
    dec_tokens = np.array(dec_tokens)
    dec_labels = np.array(dec_labels)
    return enc_tokens, dec_tokens, dec_labels


class NMTModel(tf.keras.Model):
    """
    NMTModel Class
    """
    def __init__(self, args, name='nmt_model'):
        """
        생성자
        :param args: Args 객체
        :param name: layer name
        """
        super().__init__(name=name)

        self.embedding = tf.keras.layers.Embedding(args.n_vocab, args.d_model, mask_zero=True)
        self.enc_lstm = tf.keras.layers.LSTM(units=args.d_model, return_state=True)
        self.dec_lstm = tf.keras.layers.LSTM(units=args.d_model, return_sequences=True)
        self.linear = tf.keras.layers.Dense(args.n_vocab)

    def call(self, inputs, training=False):
        """
        layer 실행
        :param inputs: tokens
        :return y_pred: tokens 예측 결과
        """
        enc_tokens, dec_tokens = inputs

        enc_hidden = self.embedding(enc_tokens)  # (bs, n_seq, d_model)
        enc_mask = self.embedding.compute_mask(enc_tokens)
        enc_hidden, enc_h_state, enc_c_state = self.enc_lstm(enc_hidden, mask=enc_mask)

        dec_hidden = self.embedding(dec_tokens)  # (bs, n_seq, d_model)
        dec_mask = self.embedding.compute_mask(dec_tokens)
        dec_hidden = self.dec_lstm(dec_hidden, initial_state=[enc_h_state, enc_c_state], mask=dec_mask)

        logits = self.linear(dec_hidden)
        return logits


#
# Train
#
EPOCHS = 50
BATCH = 128


early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=3)
save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(sychat_dir, "lstm.hdf5"),
                                                  monitor='lm_acc',
                                                  verbose=1,
                                                  save_best_only=True,
                                                  mode="max",
                                                  save_freq='epoch',
                                                  save_weights_only=True)
csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(sychat_dir, "lstm.csv"))


# 모델 학습
history = model.fit((train_enc_tokens, train_dec_tokens), train_dec_labels,
                    epochs=EPOCHS,
                    batch_size=BATCH,
                    callbacks=[early_stopping, save_weights, csv_logger])


plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], 'b-', label='loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['lm_acc'], 'g-', label='accuracy')
plt.legend()

plt.show()


def do_chat(vocab, model, n_dec_seq, string):
    """
    seq2seq chat
    :param vocab: vocab
    :param model: model
    :param n_dec_seq: number of dec seqence
    :param string: inpust string
    """
    enc_token = vocab.encode_as_ids(string)
    dec_token = [vocab.bos_id()]
    # 처음부터 예측
    start_idx = 0

    for _ in range(start_idx, n_dec_seq ):
        outputs = model((np.array([enc_token]), np.array([dec_token])))
        prob = outputs[0][start_idx]
        word_id = int(np.argmax(prob))
        if word_id == vocab.eos_id():
            break
        dec_token.append(word_id)
        start_idx += 1
    
    predict_id = dec_token[1:]
    predict_str = vocab.decode_ids(predict_id)

    return predict_str


# 모델 loss, optimizer, metric 정의
optimizer = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
acc_fn = tf.keras.metrics.SparseCategoricalAccuracy(name='lm_acc')
model.compile(loss=loss_fn, optimizer=optimizer, metrics=[acc_fn])



class NMTModel(tf.keras.Model):
    """
    NMTModel Class
    """
    def __init__(self, args, name='nmt_model'):
        """
        생성자
        :param args: Args 객체
        :param name: layer name
        """
        super().__init__(name=name)

        self.embedding = tf.keras.layers.Embedding(args.n_vocab, args.d_model, mask_zero=True)
        self.enc_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=args.d_model, return_state=True))
        self.dec_lstm = tf.keras.layers.LSTM(units=args.d_model * 2, return_sequences=True)
        self.linear = tf.keras.layers.Dense(args.n_vocab)

    def call(self, inputs, training=False):
        """
        layer 실행
        :param inputs: tokens
        :return y_pred: tokens 예측 결과
        """
        enc_tokens, dec_tokens = inputs

        enc_hidden = self.embedding(enc_tokens)  # (bs, n_seq, d_model)
        enc_mask = self.embedding.compute_mask(enc_tokens)
        enc_hidden, enc_fh, enc_fc, enc_bh, enc_bc = self.enc_lstm(enc_hidden, mask=enc_mask)
        enc_h = tf.concat([enc_fh, enc_bh], axis=-1)
        enc_c = tf.concat([enc_fc, enc_bc], axis=-1)

        dec_hidden = self.embedding(dec_tokens)  # (bs, n_seq, d_model)
        dec_mask = self.embedding.compute_mask(dec_tokens)
        dec_hidden = self.dec_lstm(dec_hidden, initial_state=[enc_h, enc_c], mask=dec_mask)

        logits = self.linear(dec_hidden)
        return logits

