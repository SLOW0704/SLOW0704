class LanguageModel(tf.keras.Model):
    """
    LanguageModel Class
    """
    def __init__(self, args, name='language_model'):
        """
        생성자
        :param args: Args 객체
        :param name: layer name
        """
        super().__init__(name=name)

        self.embedding = tf.keras.layers.Embedding(args.n_vocab, args.d_model, mask_zero=True)
        self.lstm = tf.keras.layers.LSTM(units=args.d_model, return_sequences=True)
        self.linear = tf.keras.layers.Dense(len(vocab))

    def call(self, inputs, training=False):
        """
        layer 실행
        :param inputs: tokens
        :return y_pred: tokens 예측 결과
        """
        tokens = inputs

        hidden = self.embedding(tokens)  # (bs, n_seq, d_model)
        mask = self.embedding.compute_mask(tokens)  # (bs, n_seq)
        hidden = self.lstm(hidden, mask=mask)  # (bs, n_seq, d_model)
        logits = self.linear(hidden)

        return logits


doc = []
with zipfile.ZipFile(kowiki_file) as z:
    with z.open("kowiki.txt") as f:
        for i, line in enumerate(f):
            line = line.decode('utf-8').strip()
            if line:
                doc.append(line)
            else:
                break
doc



def make_instances(doc, vocab, n_seq):
    instances = []
    n_max = n_seq - 1  # [BOS] or [EOS]

    chunk, chunk_len = [], 0
    for i, line in enumerate(doc):
        tokens = vocab.encode_as_pieces(line)
        if tokens:
            chunk.append(tokens)
            chunk_len += len(tokens)

        if 0 < len(chunk) and (n_max <= chunk_len or i >= len(doc) - 1):
            # print(chunk_len, chunk)
            token = []
            for row in chunk:
                token.extend(row)
            token = token[:n_max]
            # print(len(token), token)

            instance = {'token': token}
            instances.append(instance)

            chunk, chunk_len = [], 0

    return instances


def make_tfrecord(vocab, instance, n_seq):
    value = vocab.piece_to_id(instance['token'])
    token = [vocab.bos_id()] + value
    token += [0] * (n_seq - len(token))
    label = value + [vocab.eos_id()]
    label += [0] * (n_seq - len(label))
    # store value
    feature = {
        'token': tf.train.Feature(int64_list=tf.train.Int64List(value=token)),
        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label)),
    }
    features = tf.train.Features(feature=feature)
    example = tf.train.Example(features=features)

    return example



count = 0
options = tf.io.TFRecordOptions(compression_type='GZIP')
with tf.io.TFRecordWriter('lm-128.tfrecords', options=options) as writer:
    doc = []
    with zipfile.ZipFile(kowiki_file) as z:
        with z.open('kowiki.txt') as f:
            for i, line in enumerate(tqdm(f, total=total)):
                line = line.decode('utf-8').strip()
                if line:
                    doc.append(line)
                else:
                    if doc:
                        instances = make_instances(doc, vocab, 128)
                        for instance in instances:
                            example = make_tfrecord(vocab, instance, 128)
                            writer.write(example.SerializeToString())
                            count += 1
                    doc = []
            if doc:
                instances = make_instances(doc, vocab, 128)
                for instance in instances:
                    example = make_tfrecord(vocab, instance, 128)
                    writer.write(example.SerializeToString())
                    count += 1
count



@tf.function
def lm_loss(y_true, logits):
    """
    pad 부분을 제외하고 loss를 계산하는 함수
    :param y_true: 정답
    :param logits: 예측 값
    :retrun loss: pad 부분이 제외된 loss 값
    """
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
    loss = loss_fn(y_true, logits)
    mask = tf.cast(tf.not_equal(y_true, 0), loss.dtype)
    loss *= mask
    loss = tf.reduce_sum(loss) / tf.maximum(tf.reduce_sum(mask), 1)
    return loss


@tf.function
def lm_acc(y_true, logits):
    """
    pad 부분을 제외하고 accuracy를 계산하는 함수
    :param y_true: 정답
    :param logits: 예측 값
    :retrun loss: pad 부분이 제외된 accuracy 값
    """
    y_pred_class = tf.cast(tf.argmax(logits, axis=-1), y_true.dtype)
    matches = tf.cast(tf.equal(y_true, y_pred_class), y_true.dtype)
    mask = tf.cast(tf.not_equal(y_true, 0), matches.dtype)
    matches *= mask
    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)
    return accuracy


class LanguageModel(tf.keras.Model):
    """
    LanguageModel Class
    """
    def __init__(self, args, name='language_model'):
        """
        생성자
        :param args: Args 객체
        :param name: layer name
        """
        super().__init__(name=name)

        self.embedding = tf.keras.layers.Embedding(args.n_vocab, args.d_model, mask_zero=True)
        self.lstm = tf.keras.layers.LSTM(units=args.d_model, return_sequences=True)
        self.linear = tf.keras.layers.Dense(len(vocab))

    def call(self, inputs, training=False):
        """
        layer 실행
        :param inputs: tokens
        :return y_pred: tokens 예측 결과
        """
        tokens = inputs

        hidden = self.embedding(tokens)  # (bs, n_seq, d_model)
        mask = self.embedding.compute_mask(tokens)  # (bs, n_seq)
        hidden = self.lstm(hidden, mask=mask)  # (bs, n_seq, d_model)
        logits = self.linear(hidden)  # (bs, n_seq, n_vocab)

        return logits



def do_next(vocab, model, string):
    token = vocab.encode_as_ids(string)
    start_idx = len(token)
    token = [vocab.bos_id()] + token

    logits = model(np.array([token]))
    y_pred = tf.nn.softmax(logits, axis=-1)
    prob = y_pred[0][start_idx].numpy()
    max_args = np.argsort(prob * -1)[:10]

    next_prob = []
    for i in max_args:
        w = vocab.id_to_piece(int(i))
        p = prob[i]
        next_prob.append((w, p))
    next_prob
    
    return next_prob



def do_generate(vocab, model, n_seq, string):
    token = vocab.encode_as_ids(string)
    start_idx = len(token)
    token = [vocab.bos_id()] + token

    for _ in range(start_idx, n_seq):
        logits = model(np.array([token]))
        y_pred = tf.nn.softmax(logits, axis=-1)
        prob = y_pred[0][start_idx].numpy()
        word_id = int(np.random.choice(len(vocab), 1, p=prob)[0])
        # word_id = int(np.argmax(prob))
        # print(start_idx, word_id, token)
        if word_id == vocab.eos_id():
            break
        token.append(word_id)
        start_idx += 1
    
    predict_id = token[1:]
    predict_str = vocab.decode_ids(predict_id)
    
    return predict_str


