- 
    
    ```python
    y_true = np.random.randint(1, 3, (10, 4))
    for i in range(len(y_true)):
        y_true[i][i % 3 + 1:] = 0
    y_true
    
    y_pred = np.random.randn(10, 4, 3)
    y_pred
    
    def make_data(df, vocab, enc_len, dec_len):
        dec_max = dec_len - 1
        enc_tokens, dec_tokens, dec_labels = [], [], []
    
        prefix = vocab.encode_as_ids('감정분류:')
    
        for i, row in tqdm(df.iterrows(), total=len(df)):
            label = row["label"]
            document = row["document"]
            
            enc_token = prefix + vocab.encode_as_ids(document)
            enc_token = enc_token[:enc_len]
            enc_token += [0] * (enc_len - len(enc_token))
            enc_tokens.append(enc_token)
    
            label_id = vocab.encode_as_ids(id_to_label[label])
    
            dec_token = [vocab.bos_id()] + label_id
            dec_tokens.append(dec_token)
    
            dec_label = label_id + [vocab.eos_id()]
            dec_labels.append(dec_label)
    
        enc_tokens = np.array(enc_tokens)
        dec_tokens = np.array(dec_tokens)
        dec_labels = np.array(dec_labels)
        return enc_tokens, dec_tokens, dec_labels
    
    #
    # 평가 #
    #
    
    model = T5MultiTask(args)
    model((np.array([[0]]), np.array([[0]])))
    model.load_weights(os.path.join(nsmc_dir, "t5-no-pre.hdf5"))
    
    optimizer = tf.keras.optimizers.Adam(2.5e-4)
    
    model.compile(loss=lm_loss, optimizer=optimizer, metrics=[cls_acc])
    
    model.evaluate((test_enc_tokens, test_dec_tokens), test_dec_labels, batch_size=BATCHS)
    
    test_dataset = tf.data.Dataset.from_tensor_slices((test_enc_tokens, test_dec_tokens, test_dec_labels)).batch(BATCHS)
    test_dataset
    
    true_class, pred_class = [], []
    
    for enc_tokens, dec_tokens, dec_labels in tqdm(test_dataset):
        y_pred = model((enc_tokens, dec_tokens))
        y_pred = y_pred.numpy()
        y_pred_class = tf.argmax(y_pred, axis=-1).numpy().astype(np.int32)
        for true_id, pred_id in zip(dec_labels, y_pred_class):
            true_str = vocab.decode_ids([int(v) for v in true_id])
            true_class.append(label_to_id[true_str])
            pred_str = vocab.decode_ids([int(v) for v in pred_id])
            pred_class.append(label_to_id[pred_str])
    
    cf_matrix = np.zeros((2, 2)).astype(np.int)
    cf_matrix
    
    for y_true, y_pred in zip(true_class, pred_class):
        cf_matrix[int(y_true), int(y_pred)] += 1
    cf_matrix
    
    tp = cf_matrix[1, 1]
    tn = cf_matrix[0, 0]
    fp = cf_matrix[0, 1]
    fn = cf_matrix[1, 0]
    
    accuracy = (tp + tn) / max((tp + tn + fp + fn), 1)
    print(f'accuracy: {accuracy}')
    precision = (tp) / max((tp + fp), 1)
    print(f'precision: {precision}')
    recall = (tp) / max((tp + fn), 1)
    print(f'recall: {recall}')
    f1 = 2 * (precision * recall) / max((precision + recall), 1)
    print(f'f1: {f1}')
    
    ```
    
- T5-sts
    
    ```python
    def make_data(df, vocab, enc_len, dec_len):
        enc_tokens, dec_tokens, dec_labels = [], [], []
    
        prefix1 = vocab.encode_as_ids('유사도 문장1:')
        prefix2 = vocab.encode_as_ids('문장2:')
        prefix_len = len(prefix1) + len(prefix2)
        n_max = enc_len - prefix_len
    
        for i, row in tqdm(df.iterrows(), total=len(df)):
            label = row["label"]
            sentence1 = row["sentence1"]
            sentence2 = row["sentence2"]
    
            sentence1 = vocab.encode_as_ids(sentence1)
            sentence2 = vocab.encode_as_ids(sentence2)
            label = vocab.encode_as_ids(str(label))
            
            while len(sentence1) + len(sentence2) > n_max:
                if len(sentence1) > len(sentence2):
                    sentence1.pop()
                else:
                    sentence2.pop()
            
            enc_token = prefix1 + sentence1 + prefix2 + sentence2
            enc_token += [0] * (enc_len - len(enc_token))
            enc_tokens.append(enc_token)
    
            dec_token = [vocab.bos_id()] + label
            dec_token += [0] * (dec_len - len(dec_token))
            dec_tokens.append(dec_token)
    
            dec_label = label + [vocab.eos_id()]
            dec_label += [0] * (dec_len - len(dec_label))
            dec_labels.append(dec_label)
    
        enc_tokens = np.array(enc_tokens)
        dec_tokens = np.array(dec_tokens)
        dec_labels = np.array(dec_labels)
        return enc_tokens, dec_tokens, dec_labels
    
    model = T5MultiTask(args)
    model((np.array([[0]]), np.array([[0]])))
    model.load_weights(os.path.join(kor_sts, "t5-with-pre.hdf5"))
    
    optimizer = tf.keras.optimizers.Adam(2.5e-4)
    
    model.compile(loss=lm_loss, optimizer=optimizer, metrics=[cls_acc])
    
    model.evaluate((test_enc_tokens, test_dec_tokens), test_dec_labels, batch_size=BATCHS)
    
    test_dataset = tf.data.Dataset.from_tensor_slices((test_enc_tokens, test_dec_tokens, test_dec_labels)).batch(BATCHS)
    test_dataset
    
    true_class, pred_class = [], []
    
    for enc_tokens, dec_tokens, dec_labels in tqdm(test_dataset):
        y_pred = model((enc_tokens, dec_tokens))
        y_pred = y_pred.numpy()
        y_pred_class = tf.argmax(y_pred, axis=-1).numpy().astype(np.int32)
        for true_id, pred_id in zip(dec_labels, y_pred_class):
            true_str = vocab.decode_ids([int(v) for v in true_id])
            true_class.append(float(true_str))
            pred_str = vocab.decode_ids([int(v) for v in pred_id])
            try:
                pred_class.append(float(pred_str))
            except:
                pred_class.append(0.0)
                print(pred_str)
    
    xy_result = []
    for x, y in zip(true_class, pred_class):
        xy_result.append((x, y))
    xy_result
    
    # chart
    plt.figure(figsize=(8, 8))
    for xy in xy_result:
        plt.scatter(xy[0], xy[1], color="r")
    plt.grid()
    plt.show()
    
    import scipy.stats as stats
    stats.spearmanr(xy_result)
    
    ```
